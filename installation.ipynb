{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02338e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.env', '.git', '.mypy_cache', '.vscode', 'data', 'er2m_agents', 'img', 'installation.ipynb', 'LICENSE', 'README.md', 'requirements.txt', 'setup.py']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# =========================\n",
    "# Date parsing helpers\n",
    "# =========================\n",
    "DASH_PATTERN = re.compile(r\"\\s*[\\-–—]\\s*\")  # hyphen, en dash, em dash\n",
    "DATE_TOKEN   = re.compile(\n",
    "    r\"(\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}|\\d{1,2}\\s*[A-Za-z]{3,9}\\s*'\\d{2,4}|\\d{1,2}\\s*[A-Za-z]{3,9}\\s*\\d{2,4})\"\n",
    ")\n",
    "\n",
    "POSSIBLE_FMTS = [\n",
    "    \"%d.%m.%Y\", \"%d.%m.%y\", \"%d/%m/%Y\", \"%d/%m/%y\", \"%d-%m-%Y\", \"%d-%m-%y\",\n",
    "    \"%d %b %Y\", \"%d %b '%y\", \"%d %B %Y\", \"%d %B '%y\"\n",
    "]\n",
    "\n",
    "def parse_single_date(token: str):\n",
    "    \"\"\"Try multiple formats; fall back to pandas (day-first).\"\"\"\n",
    "    token = str(token).strip().replace(\"\\u2009\", \" \")\n",
    "    for fmt in POSSIBLE_FMTS:\n",
    "        try:\n",
    "            dt = datetime.strptime(token, fmt)\n",
    "            if dt.year < 100:  # normalize 2-digit years to 2000s\n",
    "                dt = dt.replace(year=2000 + dt.year)\n",
    "            return pd.Timestamp(dt)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.to_datetime(token, dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "def split_range(cell):\n",
    "    \"\"\"Return (start, finish) parsed from a range string or single date.\"\"\"\n",
    "    if pd.isna(cell):\n",
    "        return (pd.NaT, pd.NaT)\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return (pd.NaT, pd.NaT)\n",
    "\n",
    "    # 1) Try splitting by any dash\n",
    "    parts = DASH_PATTERN.split(s)\n",
    "    if len(parts) == 2:\n",
    "        return (parse_single_date(parts[0]), parse_single_date(parts[1]))\n",
    "\n",
    "    # 2) Otherwise, take the first 1–2 recognizable date tokens\n",
    "    tokens = DATE_TOKEN.findall(s)\n",
    "    if len(tokens) >= 2:\n",
    "        return (parse_single_date(tokens[0]), parse_single_date(tokens[1]))\n",
    "    if len(tokens) == 1:\n",
    "        return (parse_single_date(tokens[0]), pd.NaT)\n",
    "    return (pd.NaT, pd.NaT)\n",
    "\n",
    "# =========================\n",
    "# File/folder utilities\n",
    "# =========================\n",
    "def pick_latest_file(\n",
    "    folder_path: str,\n",
    "    name_contains: Optional[str] = None,\n",
    "    allowed_exts: Tuple[str, ...] = (\".xlsx\", \".csv\"),\n",
    ") -> str:\n",
    "    \"\"\"Return full path of the newest matching file in a SharePoint-synced folder.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise NotADirectoryError(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "    candidates: List[Tuple[str, float]] = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        lower = fname.lower()\n",
    "        if not lower.endswith(allowed_exts):\n",
    "            continue\n",
    "        if name_contains and name_contains.lower() not in lower:\n",
    "            continue\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        if os.path.isfile(fpath):\n",
    "            candidates.append((fpath, os.path.getmtime(fpath)))\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No matching files in {folder_path} \"\n",
    "            f\"(filters: name_contains={name_contains}, exts={allowed_exts})\"\n",
    "        )\n",
    "\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0]\n",
    "\n",
    "# =========================\n",
    "# Header detection\n",
    "# =========================\n",
    "def detect_header_row(df0: pd.DataFrame, must_have=(\"Banner Group\",\"Brand\",\"Variant\")) -> int:\n",
    "    \"\"\"Locate the header row (tables often have logos/notes above).\"\"\"\n",
    "    scan_rows = min(100, len(df0))\n",
    "    target = [m.lower() for m in must_have]\n",
    "    for i in range(scan_rows):\n",
    "        row_vals = df0.iloc[i].astype(str).str.strip().str.lower().tolist()\n",
    "        if all(any(m == v for v in row_vals) for m in target):\n",
    "            return i\n",
    "    return 0  # fallback\n",
    "\n",
    "# =========================\n",
    "# Planner readers (CSV/XLSX)\n",
    "# =========================\n",
    "def _read_csv_with_fallbacks(path: str) -> pd.DataFrame:\n",
    "    for enc in (None, \"utf-8-sig\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(f\"Failed to read CSV: {path}\")\n",
    "\n",
    "def read_planner(path: str, sheet_name: str = \"PFM Promo Planner\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the planner:\n",
    "      - detect header row (Excel),\n",
    "      - apply headers,\n",
    "      - **forward-fill merged cells in `Banner Group` and `Brand`**,\n",
    "      - ensure `Variant` exists,\n",
    "      - keep only meaningful rows.\n",
    "    \"\"\"\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        raw = _read_csv_with_fallbacks(path)\n",
    "        # ---- Forward-fill fix for CSVs as well (in case of blank repeats) ----\n",
    "        for col in (\"Banner Group\", \"Brand\"):\n",
    "            if col in raw.columns:\n",
    "                raw[col] = raw[col].ffill()\n",
    "        if \"Variant\" not in raw.columns:\n",
    "            raw[\"Variant\"] = pd.NA\n",
    "        keep = ~raw[[\"Brand\",\"Variant\"]].isna().all(axis=1)\n",
    "        return raw.loc[keep].reset_index(drop=True)\n",
    "\n",
    "    # Excel workbook\n",
    "    df0 = pd.read_excel(path, sheet_name=sheet_name, header=None, engine=\"openpyxl\")\n",
    "    hdr = detect_header_row(df0)\n",
    "    headers = df0.iloc[hdr].astype(str).str.strip().tolist()\n",
    "    raw = df0.iloc[hdr + 1 :].copy()\n",
    "    raw.columns = headers\n",
    "    raw = raw.reset_index(drop=True)\n",
    "\n",
    "    # ---- Forward-fill fix for merged cells (your requested change) ----\n",
    "    for col in (\"Banner Group\", \"Brand\"):\n",
    "        if col in raw.columns:\n",
    "            raw[col] = raw[col].ffill()\n",
    "\n",
    "    # Make sure Variant exists (some tabs omit it)\n",
    "    if \"Variant\" not in raw.columns:\n",
    "        raw[\"Variant\"] = pd.NA\n",
    "\n",
    "    # Keep rows that have at least Brand or Variant\n",
    "    keep = ~raw[[\"Brand\",\"Variant\"]].isna().all(axis=1)\n",
    "    return raw.loc[keep].reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# Period extraction\n",
    "# =========================\n",
    "def _coalesce(df: pd.DataFrame, candidates: List[str]) -> Optional[pd.Series]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df[c]\n",
    "    return None\n",
    "\n",
    "def extract_periods(raw: pd.DataFrame):\n",
    "    \"\"\"Support combined range (one column) or Start/Finish (two columns).\"\"\"\n",
    "    buy_combined  = [\"Buy In Period\", \"Buy-in Period\", \"Buyin Period\"]\n",
    "    buy_start     = [\"Buy In Start\", \"Buy-in Start\", \"Start (Buy In)\",\n",
    "                     \"Buy In - Start\", \"Buy In: Start\", \"Start - Buy In\", \"Start\"]\n",
    "    buy_finish    = [\"Buy In Finish\", \"Buy-in Finish\", \"Finish (Buy In)\",\n",
    "                     \"Buy In - Finish\", \"Buy In: Finish\", \"Finish - Buy In\", \"Finish\"]\n",
    "\n",
    "    promo_combined = [\"Promotional Period\", \"Promo Period\", \"Promotion Period\"]\n",
    "    promo_start    = [\"Promotional Start\", \"Promo Start\", \"Promotion Start\",\n",
    "                      \"Start (Promo)\", \"Start - Promo\", \"Start (Promotional Period)\", \"Start\"]\n",
    "    promo_finish   = [\"Promotional Finish\", \"Promo Finish\", \"Promotion Finish\",\n",
    "                      \"Finish (Promo)\", \"Finish - Promo\", \"Finish (Promotional Period)\", \"Finish\"]\n",
    "\n",
    "    def extract(df, combined_cols, start_cols, finish_cols):\n",
    "        s_col, f_col = _coalesce(df, start_cols), _coalesce(df, finish_cols)\n",
    "        if s_col is not None or f_col is not None:\n",
    "            s = pd.to_datetime(s_col, errors=\"coerce\", dayfirst=True) if s_col is not None else pd.Series(pd.NaT, index=df.index)\n",
    "            f = pd.to_datetime(f_col, errors=\"coerce\", dayfirst=True) if f_col is not None else pd.Series(pd.NaT, index=df.index)\n",
    "            return s, f\n",
    "\n",
    "        comb = _coalesce(df, combined_cols)\n",
    "        if comb is None:\n",
    "            return pd.Series(pd.NaT, index=df.index), pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "        s_vals, f_vals = zip(*comb.apply(split_range))\n",
    "        return pd.Series(list(s_vals), index=df.index), pd.Series(list(f_vals), index=df.index)\n",
    "\n",
    "    buy_s, buy_f = extract(raw, buy_combined,  buy_start,  buy_finish)\n",
    "    pro_s, pro_f = extract(raw, promo_combined, promo_start, promo_finish)\n",
    "    return buy_s, buy_f, pro_s, pro_f\n",
    "\n",
    "# =========================\n",
    "# Build final result\n",
    "# =========================\n",
    "def build_result(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    buy_s, buy_f, pro_s, pro_f = extract_periods(raw)\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        \"Banner Group\": raw.get(\"Banner Group\", pd.Series([pd.NA]*len(raw))),\n",
    "        \"Brand\":        raw.get(\"Brand\", pd.Series([pd.NA]*len(raw))),\n",
    "        \"Variant\":      raw.get(\"Variant\", pd.Series([pd.NA]*len(raw))),\n",
    "        \"Buy In Start\": buy_s,\n",
    "        \"Buy In Finish\":buy_f,\n",
    "        \"Promo Start\":  pro_s,\n",
    "        \"Promo Finish\": pro_f\n",
    "    })\n",
    "\n",
    "    # Keep rows that have a Variant OR at least one date (avoid spacer rows)\n",
    "    date_cols = [\"Buy In Start\",\"Buy In Finish\",\"Promo Start\",\"Promo Finish\"]\n",
    "    keep = (~res[\"Variant\"].isna()) | (~pd.isna(res[date_cols]).all(axis=1))\n",
    "    res = res.loc[keep].reset_index(drop=True)\n",
    "\n",
    "    # Clean text fields\n",
    "    for c in [\"Banner Group\",\"Brand\",\"Variant\"]:\n",
    "        if c in res.columns:\n",
    "            res[c] = res[c].astype(str).str.strip().replace({\"nan\": \"\", \"None\": \"\"})\n",
    "\n",
    "    return res\n",
    "\n",
    "# =========================\n",
    "# Transformations\n",
    "# =========================\n",
    "def promo_configurator(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Final structure\n",
    "    field_list = [\n",
    "        'name',\n",
    "        'start_date',\n",
    "        'end_date',\n",
    "        'manufacturer_id',\n",
    "        'buyer_group_id',\n",
    "        'user',\n",
    "        'date_added'\n",
    "    ]\n",
    "\n",
    "    promo_config = pd.DataFrame(columns=field_list)\n",
    "\n",
    "    # Base fields\n",
    "    promo_config['name'] = df['Banner Group'].astype(str).str.strip() + \" - \" + df['Brand'].astype(str).str.strip()\n",
    "    promo_config['start_date'] = pd.to_datetime(df['Promo Start'], dayfirst=True, errors='coerce')\n",
    "    promo_config['end_date']   = pd.to_datetime(df['Promo Finish'], dayfirst=True, errors='coerce')\n",
    "    promo_config['manufacturer_id'] = 206575077\n",
    "    promo_config['user'] = 'bevco_master'\n",
    "    promo_config['date_added'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # -----------------------\n",
    "    # Buyer Group Mapping\n",
    "    # -----------------------\n",
    "    mapping = {\n",
    "        \"Sasol\": 177268,\n",
    "        \"TotalEnergies\": 178524,\n",
    "        \"BP Express\": 1864328,\n",
    "        \"Engen\": 1865071,\n",
    "        \"Shell\": 1867297,\n",
    "        \"Freshstop /Astron / Caltex\": 172785,\n",
    "        \"Pick 'n Pay Express\": 1864328\n",
    "    }\n",
    "\n",
    "    # Normalize Banner Group for matching\n",
    "    cleaned = df['Banner Group'].astype(str).str.strip()\n",
    "\n",
    "    buyer_ids = []\n",
    "    for val in cleaned:\n",
    "        assigned = None\n",
    "        # Attempt direct match\n",
    "        if val in mapping:\n",
    "            assigned = mapping[val]\n",
    "        else:\n",
    "            # Attempt substring match (e.g. \"Caltex (Astron) Caltex\" etc.)\n",
    "            for key in mapping:\n",
    "                if key in val:\n",
    "                    assigned = mapping[key]\n",
    "                    break\n",
    "\n",
    "        buyer_ids.append(assigned if assigned is not None else \"\")\n",
    "\n",
    "    promo_config['buyer_group_id'] = buyer_ids\n",
    "\n",
    "    return promo_config\n",
    "\n",
    "# =========================\n",
    "# Main convenience entrypoint\n",
    "# =========================\n",
    "def extract_promos_from_sharepoint_folder(\n",
    "    folder_path: str,\n",
    "    name_contains: Optional[str] = None,\n",
    "    sheet_name: str = \"PFM Promo Planner\",\n",
    "    github_data_folder: str = r\"http://github.com/insight-base/promo-loading-automation/data\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Select newest .xlsx/.csv in the SharePoint-synced folder.\n",
    "    2) Read the planner tab and normalize (with forward-fill fix).\n",
    "    3) Parse date ranges.\n",
    "    4) Save a dated CSV into your GitHub repo's /data folder.\n",
    "    \"\"\"\n",
    "    latest_path = pick_latest_file(folder_path, name_contains=name_contains)\n",
    "    raw = read_planner(latest_path, sheet_name=sheet_name)\n",
    "    result = build_result(raw)\n",
    "    result = promo_configurator(result)\n",
    "    \n",
    "\n",
    "    # --- Save to GitHub repository instead of SharePoint ---\n",
    "    os.makedirs(github_data_folder, exist_ok=True)\n",
    "\n",
    "    out_name = f\"bevco_promo_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    output_path = os.path.join(github_data_folder, out_name)\n",
    "\n",
    "    result.to_csv(output_path, index=False)\n",
    "    return result, output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92184c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- config ----\n",
    "sharepoint_folder =  r\"C:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\eR2m Team Sharepoint - Eddwin\\Loading Promos\" # OneDrive-synced path\n",
    "\n",
    "df, out_csv = extract_promos_from_sharepoint_folder(\n",
    "    folder_path=sharepoint_folder,\n",
    "    name_contains=\"PFM Promotional Planner\",   # optional filter\n",
    "    sheet_name=\"PFM Promo Planner\",            # default; change if the tab name differs\n",
    "    github_data_folder=r\".\\data\"  # your local GitHub repo path\n",
    ")\n",
    "print(out_csv)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_promos_from_df(df: pd.DataFrame, mysql_cfg: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calls front_end.sp_insert_config_promo for each row in df.\n",
    "    Returns df with a new column `config_promo_id` holding AUTO_INCREMENT IDs.\n",
    "    Required df columns:\n",
    "      name, start_date, end_date, manufacturer_id, buyer_group_id, user, date_added\n",
    "    \"\"\"\n",
    "    cnx = mysql.connector.connect(**mysql_cfg)\n",
    "    cnx.autocommit = False  # we control commits\n",
    "    cur = cnx.cursor()\n",
    "\n",
    "    inserted_ids = []\n",
    "    try:\n",
    "        for i, row in df.iterrows():\n",
    "            # Ensure date types are correct (MySQL connector likes python date)\n",
    "            start_date = pd.to_datetime(row[\"start_date\"]).date() if pd.notna(row[\"start_date\"]) else None\n",
    "            end_date   = pd.to_datetime(row[\"end_date\"]).date()   if pd.notna(row[\"end_date\"]) else None\n",
    "            date_added = pd.to_datetime(row[\"date_added\"]).date() if \"date_added\" in df.columns and pd.notna(row[\"date_added\"]) else date.today()\n",
    "\n",
    "            args = (\n",
    "                str(row[\"name\"]),\n",
    "                start_date,\n",
    "                end_date,\n",
    "                int(row[\"manufacturer_id\"]),\n",
    "                int(row[\"buyer_group_id\"]),\n",
    "                str(row[\"user\"]),\n",
    "                date_added,\n",
    "                0  # placeholder for OUT param (some connectors require it even if unused)\n",
    "            )\n",
    "\n",
    "            # 1) assign OUT param to a session variable\n",
    "            cur.execute(\"SET @o_config_promo_id = LAST_INSERT_ID();\")\n",
    "\n",
    "            # 2) call procedure passing @o_config_promo_id as OUT\n",
    "            cur.callproc(\"supply_chain.insert_config_promo\", [\n",
    "                args[0], args[1], args[2], args[3], args[4], args[5], args[6], \"@o_config_promo_id\", \"@o_message\"\n",
    "            ])\n",
    "\n",
    "            # 3) read the OUT value\n",
    "            print(f\"Running the stored procedure for row {i+1}/{len(df)}: {args[0]}\")\n",
    "            cur.execute(\"SELECT @o_config_promo_id;\")\n",
    "            new_id = cur.fetchone()[0]\n",
    "            inserted_ids.append(int(new_id))\n",
    "\n",
    "        cnx.commit()\n",
    "    except Exception as e:\n",
    "        cnx.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        cnx.close()\n",
    "        print(\"MySQL connection closed.\")\n",
    "\n",
    "    out_df = df.copy()\n",
    "    out_df[\"config_promo_id\"] = inserted_ids\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MYSQL_HOST=os.getenv(\"MYSQL_HOST\")\n",
    "MYSQL_PORT=os.getenv(\"MYSQL_PORT\")\n",
    "MYSQL_USER=os.getenv(\"MYSQL_USER\")\n",
    "MYSQL_PASS=os.getenv(\"MYSQL_PASS\")\n",
    "MYSQL_DB=os.getenv(\"front_end\")\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    \"name\": \"Engen - Pepsi 500ml - 2026/01/22 - 2026/03/11\",\n",
    "    \"start_date\": \"2026-03-01\",\n",
    "    \"end_date\": \"2026-04-30\",\n",
    "    \"manufacturer_id\": 206575077,\n",
    "    \"buyer_group_id\": 1865071,\n",
    "    \"user\": \"bevco_master\",\n",
    "    \"date_added\": \"2026-01-26\"\n",
    "}])\n",
    "\n",
    "mysql_cfg = {\n",
    "    \"host\": os.getenv(\"MYSQL_HOST\"),\n",
    "    \"user\": os.getenv(\"MYSQL_USER\"),\n",
    "    \"password\": os.getenv(\"MYSQL_PASS\"),\n",
    "    \"database\": os.getenv(\"front_end\"),\n",
    "    \"port\": os.getenv(\"MYSQL_PORT\")\n",
    "}\n",
    "\n",
    "df_with_ids = load_promos_from_df(df, mysql_cfg)\n",
    "df_with_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
